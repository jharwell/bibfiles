@article{repro:Gundersen2022,
title = {Do machine learning platforms provide out-of-the-box reproducibility?},
journal = {Future Generation Computer Systems},
volume = {126},
pages = {34-47},
year = {2022},
issn = {0167-739X},
nodoi = {https://doi.org/10.1016/j.future.2021.06.014},
nourl = {https://www.sciencedirect.com/science/article/pii/S0167739X21002090},
author = {Odd Erik Gundersen and Saeid Shamsaliei and Richard Juul Isdahl},
keywords = {Reproducibility, Reproducible AI, Machine learning, Survey, Reproducibility experiment},
abstract = {Science is experiencing an ongoing reproducibility crisis. In light of this crisis, our objective is to investigate whether machine learning platforms provide out-of-the-box reproducibility. Our method is twofold: First, we survey machine learning platforms for whether they provide features that simplify making experiments reproducible out-of-the-box. Second, we conduct the exact same experiment on four different machine learning platforms, and by this varying the processing unit and ancillary software only. The survey shows that no machine learning platform supports the feature set described by the proposed framework while the experiment reveals statstically significant difference in results when the exact same experiment is conducted on different machine learning platforms. The surveyed machine learning platforms do not on their own enable users to achieve the full reproducibility potential of their research. Also, the machine learning platforms with most users provide less functionality for achieving it. Furthermore, results differ when executing the same experiment on the different platforms. Wrong conclusions can be inferred at the at 95% confidence level. Hence, we conclude that machine learning platforms do not provide reproducibility out-of-the-box and that results generated from one machine learning platform alone cannot be fully trusted.}
}

@Article{repro:Bellogin2021,
author={Bellog{\'i}n, Alejandro and Said, Alan},
title={Improving accountability in recommender systems research through reproducibility},
journal={User Modeling and User-Adapted Interaction},
year={2021},
month={Nov},
day={01},
volume={31},
number={5},
pages={941-977},
abstract={Reproducibility is a key requirement for scientific progress. It allows the reproduction of the works of others, and, as a consequence, to fully trust the reported claims and results. In this work, we argue that, by facilitating reproducibility of recommender systems experimentation, we indirectly address the issues of accountability and transparency in recommender systems research from the perspectives of practitioners, designers, and engineers aiming to assess the capabilities of published research works. These issues have become increasingly prevalent in recent literature. Reasons for this include societal movements around intelligent systems and artificial intelligence striving toward fair and objective use of human behavioral data (as in Machine Learning, Information Retrieval, or Human--Computer Interaction). Society has grown to expect explanations and transparency standards regarding the underlying algorithms making automated decisions for and around us. This work surveys existing definitions of these concepts and proposes a coherent terminology for recommender systems research, with the goal to connect reproducibility to accountability. We achieve this by introducing several guidelines and steps that lead to reproducible and, hence, accountable experimental workflows and research. We additionally analyze several instantiations of recommender system implementations available in the literature and discuss the extent to which they fit in the introduced framework. With this work, we aim to shed light on this important problem and facilitate progress in the field by increasing the accountability of research.},
issn={1573-1391},
nodoi={10.1007/s11257-021-09302-x},
nourl={https://doi.org/10.1007/s11257-021-09302-x}
}

@article{repro:Gundersen2019,
author = {Gundersen, Odd Erik},
year = {2019},
month = {12},
pages = {9-23},
title = {Standing on the Feet of Giants — Reproducibility in AI},
volume = {40},
journal = {AI Magazine},
nodoi = {10.1609/aimag.v40i4.5185}
}

@INPROCEEDINGS{repro:SwarmRob2018,
  author={Pörtner, Aljoscha and Hoffmann, Martin and Zug, Sebastian and Knig, Matthias},
  booktitle={Proc. IEEE Int'l Conf. on Systems, Man, and Cybernetics (SMC)},
  title ={{Swarmrob:} A
docker-based toolkit for reproducibility and sharing of experimental artifacs in robotics research},
  year={2018},
  volume={},
  number={},
  pages={325--332},
  nodoi={10.1109/SMC.2018.00065}}

@INPROCEEDINGS{repro:Afzal2021,
  author={Afzal, Afsoon and Katz, Deborah S. and Le Goues, Claire and Timperley, Christopher S.},
  booktitle={2021 14th IEEE Conference on Software Testing, Verification and Validation (ICST)},
  title={Simulation for Robotics Test Automation: Developer Perspectives},
  year={2021},
  volume={},
  number={},
  pages={263-274},
  nodoi={10.1109/ICST49551.2021.00036}}

@InProceedings{repro:DataFed2020,
author="Stansberry, Dale and Somnath, Suhas and Shutt, Gregory and Shankar, Mallikarjun",
editor="Nichols, Jeffrey and Verastegui, Becky and Maccabe, Arthur and Hernandez, Oscar and Parete-Koon, Suzanne and Ahearn, Theresa",
title="A Systemic Approach to Facilitating Reproducibility via Federated, End-to-End Data Management",
booktitle="Driving Scientific and Engineering Discoveries Through the Convergence of HPC, Big Data and AI",
year="2020",
publisher="Springer Int'l Publishing",
noaddress="Cham",
pages="83--98",
abstract="Advances in computing infrastructure and instrumentation have accelerated scientific discovery in addition to exploding the data volumes. Unfortunately, the unavailability of equally advanced data management infrastructure has led to ad hoc practices that diminish scientific productivity and exacerbate the reproducibility crisis. We discuss a system-wide solution that supports management needs at every stage of the data lifecycle. At the center of this system is DataFed - a general purpose, scientific data management system that addresses these challenges by federating data storage across facilities with central metadata and provenance management - providing simple and uniform data discovery, access, and collaboration capabilities. At the edge is a Data Gateway that captures raw data and context from experiments (even when performed on off-network instruments) into DataFed. DataFed can be integrated into analytics platforms to easily, correctly, and reliably work with datasets to improve reproducibility of such workloads. We believe that this system can significantly alleviate the burden of data management and improve compliance with the Findable Accessible Interoperable, Reusable (FAIR) data principles, thereby improving scientific productivity and rigor.",
isbn="978-3-030-63393-6"
}

@article{repro:CodeOcean2019,
  title={Computational reproducibility via containers in social psychology},
  author={Clyburne-Sherin, April and Fei, Xu and Green, Seth Ariel},
  journal={Meta-Psychology},
  volume={3},
  year={2019},
  publisher={LnuOpen}
}

@article{repro:Gundersen2018,
title={On Reproducible AI: Towards Reproducible Research, Open
                  Science, and Digital Scholarship in AI
                  Publications},
volume={39},
nourl={https://ojs.aaai.org/index.php/aimagazine/article/view/2816},
nodoi={10.1609/aimag.v39i3.2816},
number={3},
journal={AI Magazine},
author={Gundersen, Odd Erik and Gil, Yolanda and Aha, David W.},
year={2018},
month={Sep.},
pages={56-68}
}
